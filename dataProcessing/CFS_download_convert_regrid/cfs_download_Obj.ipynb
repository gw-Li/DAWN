{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88880ad2-49d2-4e75-b7dc-7d90d7145531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import subprocess\n",
    "import datetime \n",
    "import cfgrib\n",
    "import requests\n",
    "import xarray   as xr\n",
    "import xesmf    as xe\n",
    "from multiprocessing import Pool\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc5be36-c9e3-43ee-ba7c-266d380393e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download succeeded: tmp2m.01.2011050100.daily.grb2\n",
      "Download succeeded: tmp2m.01.2012050100.daily.grb2\n",
      "Download succeeded: tmp2m.01.2013050100.daily.grb2\n",
      "Download succeeded: tmp2m.01.2014050100.daily.grb2\n",
      "Download succeeded: tmp2m.01.2015050100.daily.grb2\n",
      "Download succeeded: tmp2m.01.2016050100.daily.grb2\n",
      "Download succeeded: tmp2m.01.2017050100.daily.grb2\n",
      "Download succeeded: tmp2m.01.2018050100.daily.grb2\n",
      "Download succeeded: tmp2m.01.2019050100.daily.grb2\n",
      "Download succeeded: tmp2m.01.2020050100.daily.grb2\n",
      "Download succeeded: tmp2m.01.2021050100.daily.grb2\n",
      "Download succeeded: tmp2m.01.2022050100.daily.grb2\n"
     ]
    }
   ],
   "source": [
    "# Modify the begin and end month if necessary, as well as years and the file directory.\n",
    "beg_month_int,beg_day_int,end_month_int, end_day_int = 6,1,8,31\n",
    "init_month_int,  init_day_int, init_hour_int  = 5,1,0\n",
    "years    = range(2011,2023)\n",
    "data_dir = '/home/umd-gwli/scratch16-umd-xliang/gwli/DATA/CFS/'\n",
    "stic_dir = '/home/umd-gwli/scratch16-umd-xliang/gwli/DATA/static/'\n",
    "TIMEFMT  = '%Y-%m-%d-%H'\n",
    "\n",
    "# Add more variables if necessary.\n",
    "var_names= ['PRAVG','T2MAX','T2MIN','ASWDNS','AQ2M','SOILT1','SOILM1','SOILM2','SOILM3','SOILM4','AT2M',]\n",
    "\n",
    "\n",
    "varname_mapping = {\n",
    "    \"PRAVG\": {\n",
    "        \"filename_fragment\" :  \"prate\",\n",
    "        \"original_var_name\" :  \"prate\",\n",
    "    },\n",
    "    \"T2MAX\": {\n",
    "        \"filename_fragment\" :  \"tmax\",\n",
    "        \"original_var_name\" :  \"tmax\",\n",
    "    },\n",
    "    \"T2MIN\": {\n",
    "        \"filename_fragment\" :  \"tmin\",\n",
    "        \"original_var_name\" :  \"tmin\",\n",
    "    },\n",
    "    \"ASWDNS\": {\n",
    "        \"filename_fragment\" :  \"dswsfc\",\n",
    "        \"original_var_name\" :  \"dswrf\",\n",
    "    },\n",
    "    \"AT2M\": {\n",
    "        \"filename_fragment\" :  \"tmp2m\",\n",
    "        \"original_var_name\" :  \"t2m\",\n",
    "    },\n",
    "    \"AQ2M\": {\n",
    "        \"filename_fragment\" :  \"q2m\",\n",
    "        \"original_var_name\" :  \"sh2\",\n",
    "    },\n",
    "    \"SOILT1\": {\n",
    "        \"filename_fragment\" :  \"soilt1\",\n",
    "        \"original_var_name\" :  \"t\",\n",
    "    },\n",
    "    \"SOILM1\": {\n",
    "        \"filename_fragment\" :  \"soilm1\",\n",
    "        \"original_var_name\" :  \"soilw\",\n",
    "    },\n",
    "    \"SOILM2\": {\n",
    "        \"filename_fragment\" :  \"soilm2\",\n",
    "        \"original_var_name\" :  \"soilw\",\n",
    "    },\n",
    "    \"SOILM3\": {\n",
    "        \"filename_fragment\" :  \"soilm3\",\n",
    "        \"original_var_name\" :  \"soilw\",\n",
    "    },\n",
    "    \"SOILM4\": {\n",
    "        \"filename_fragment\" :  \"soilm4\",\n",
    "        \"original_var_name\" :  \"soilw\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Specify the algorithm for calculating daily\n",
    "def calculate_daily_values(data, var_name):\n",
    "    if var_name == 'T2MIN':\n",
    "        dailydata =  data.resample(time='1D').min()\n",
    "        dailydata.attrs['units'] = data.attrs['units']\n",
    "        return dailydata\n",
    "    elif var_name == 'T2MAX':\n",
    "        dailydata =  data.resample(time='1D').max()\n",
    "        dailydata.attrs['units'] = data.attrs['units']\n",
    "        return dailydata\n",
    "    elif var_name == 'PRAVG':\n",
    "        dailydata = data.resample(time='1D').mean() * 86400\n",
    "        dailydata.attrs['units'] = 'mm/d'\n",
    "        return  dailydata\n",
    "    else:\n",
    "        dailydata =  data.resample(time='1D').mean()\n",
    "        dailydata.attrs['units'] = data.attrs['units']\n",
    "        return  dailydata\n",
    "        \n",
    "\n",
    "# Define a class for Data preparation and downloading\n",
    "class DataDownloader:\n",
    "    def __init__(self, var_name, time_init, data_dir):\n",
    "        self.var_name   = var_name\n",
    "        self.time_init  = time_init\n",
    "        self.data_dir   = data_dir\n",
    "        self.link_remot = 'https://www.ncei.noaa.gov/data/climate-forecast-system/access/operational-9-month-forecast/time-series/'\n",
    "\n",
    "    def assemble_filenames(self):\n",
    "        init_year, init_month, init_day, init_hour = self.time_init.split(\"-\")\n",
    "        file_name_original = varname_mapping[self.var_name]['filename_fragment'] + \".01.\" +  str(init_year) + \\\n",
    "                             str(init_month)+ str(init_day)+ str(init_hour) + '.daily.grb2'\n",
    "        self.file_name_original = file_name_original\n",
    "        self.file_name_raw_6h   = 'CFSraw_6h_'    + var_name +  \"_\" + time_init +'.nc'\n",
    "        self.url = self.link_remot + init_year + \"/\" + init_year + init_month + \"/\" + init_year + init_month + \\\n",
    "              init_day + \"/\" + init_year + init_month + init_day + init_hour + \"/\" + self.file_name_original\n",
    "\n",
    "    def download_and_rename(self):\n",
    "        response = requests.get(self.url, stream=True)\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError:\n",
    "            print(f\"Download failed: {response.status_code}, {response.reason}\")\n",
    "            return False\n",
    "        with open(self.data_dir + 'grib_file/' + self.file_name_original, 'wb') as fd:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                fd.write(chunk)\n",
    "        print(f\"Download succeeded: {self.file_name_original}\")\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "# Define a class for handling file conversion\n",
    "class FileConverter:\n",
    "    def __init__(self, downloader):\n",
    "        self.downloader = downloader\n",
    "\n",
    "    def convert_file(self):\n",
    "        # Convert data from grib2 file format to netcdf file format.\n",
    "        self.ds = xr.open_dataset(self.downloader.data_dir + 'grib_file/' + self.downloader.file_name_original , engine='cfgrib')\n",
    "\n",
    "        # Delete temporary files when reading grib2 files\n",
    "        os.system(f'rm {self.downloader.data_dir}grib_file/{self.downloader.file_name_original}*.idx')\n",
    "\n",
    "        # Convet the 'step' to a 'time' coordinate\n",
    "        self.ds['time'] = self.ds['time'] + self.ds['step']\n",
    "        self.ds = self.ds.set_coords('time')\n",
    "        self.ds = self.ds.swap_dims({'step': 'time'})\n",
    "\n",
    "        # Modify the variable name and coordinate name \n",
    "        self.ds = self.ds.rename({varname_mapping[self.downloader.var_name]['original_var_name']:self.downloader.var_name ,})\n",
    "\n",
    "        # Save CFS raw data\n",
    "        self.ds.to_netcdf(self.downloader.data_dir + 'raw_data/' + self.downloader.file_name_raw_6h)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class DataRegridder:\n",
    "    def __init__(self, converter, time_beg, time_end, stic_dir, var_name):\n",
    "        self.converter = converter\n",
    "        self.time_beg  = time_beg\n",
    "        self.time_end  = time_end\n",
    "        self.stic_dir  = stic_dir\n",
    "        self.var_name  = var_name\n",
    "\n",
    "    def regrid_data(self):\n",
    "        # Read in the meteorology data on a regular lat/lon grid\n",
    "        wtg_file = self.stic_dir + 'CFS2CWRF_weights_file.nc'\n",
    "        ds_in    = self.converter.ds\n",
    "\n",
    "        # Read in the CWRF grid file\n",
    "        ds_cwrf  = xr.open_dataset(self.stic_dir + 'geo_em.d01_30.nc')\n",
    "\n",
    "        # Create a new dataset with the latitude and longitude from the WRF grid\n",
    "        ds_out   = xr.Dataset({'lat': ds_cwrf['XLAT_M'].isel(Time=0), 'lon': ds_cwrf['XLONG_M'].isel(Time=0)})\n",
    "\n",
    "        # Create the regridder using the pre-generated weights\n",
    "        regridder = xe.Regridder(ds_in, ds_out, method='bilinear', filename=wtg_file, reuse_weights=True)\n",
    "\n",
    "        # Regrid the meteorology data\n",
    "        ds_in_regridded = regridder(ds_in)\n",
    "\n",
    "        # copy the attribute of units to regrided dataset.\n",
    "        ds_in_regridded[self.var_name].attrs['units'] = ds_in[self.var_name].attrs.get('units')\n",
    "\n",
    "        # Convert to daily data\n",
    "        daily_data = calculate_daily_values(ds_in_regridded[self.var_name], self.var_name)\n",
    "\n",
    "        # Trim the dataset along the 'time' dimension\n",
    "        daily_data_trim = daily_data.sel(time=slice(self.time_beg, self.time_end))\n",
    "        \n",
    "\n",
    "        # Export the result to netcdf format.\n",
    "        file_name_CFS   = 'CFS_' + var_name +  \"_\" + time_beg  + \"_\" + time_end + \"_EX_\"+ time_init +'.nc'\n",
    "        daily_data_trim.to_netcdf(self.converter.downloader.data_dir + 'regrid_daily/' + file_name_CFS)\n",
    "\n",
    "        \n",
    "        \n",
    "# Continue defining classes for each major step of the function...\n",
    "\n",
    "\n",
    "# Finally, to run the function:\n",
    "for var_name in var_names[10:11]:\n",
    "    for year in years:\n",
    "        # set time_beg and time_end for trim the data through time dimension\n",
    "        time_beg = datetime.datetime(year, beg_month_int,  beg_day_int,  0).strftime(TIMEFMT)\n",
    "        time_end = datetime.datetime(year, end_month_int,  end_day_int, 18).strftime(TIMEFMT)\n",
    "        time_init= datetime.datetime(year, init_month_int, init_day_int, init_hour_int).strftime(TIMEFMT)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialize a DataDownloader instance\n",
    "        downloader = DataDownloader(var_name, time_init, data_dir)\n",
    "        downloader.assemble_filenames()\n",
    "\n",
    "        # Download the data\n",
    "        download_success = downloader.download_and_rename()\n",
    "\n",
    "        # If the download was successful, convert and regrid the data\n",
    "        if download_success:\n",
    "            # Initialize a FileConverter instance\n",
    "            converter = FileConverter(downloader)\n",
    "            converter.convert_file()\n",
    "\n",
    "            # Initialize a DataRegridder instance\n",
    "            regridder = DataRegridder(converter, time_beg, time_end, stic_dir, var_name)\n",
    "            regridder.regrid_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bf0e7-e20e-4c10-9b5c-ffa6c5fe9562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
